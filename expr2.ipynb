{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Python Path: /home/chris/miniconda3/envs/LTN/bin/python\n",
      "* Library Version:\n",
      "  - torch 1.10.1\n",
      "  - LTNtorch 0.9\n",
      "  - transformers 4.15.0\n",
      "* Experiment ID: 2\n",
      "* Pretrained Model: monologg/koelectra-base-v3-discriminator\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from main import lang_models, TrainerForClassification, TrainerForRegression\n",
    "\n",
    "eid = 2\n",
    "python = sys.executable\n",
    "print(f\"* Python Path: {python}\")\n",
    "print(f\"* Library Version:\")\n",
    "!echo '  -' `$python -m pip list | grep -w torch`\n",
    "!echo '  -' `$python -m pip list | grep -w LTNtorch`\n",
    "!echo '  -' `$python -m pip list | grep -w transformers`\n",
    "print(f\"* Experiment ID: {eid}\")\n",
    "print(f\"* Pretrained Model: {lang_models[eid]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression using KoELECTRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### learning_rate=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-05 01:01:07] [BEGIN] TrainerForRegression.define().train()\n",
      "\n",
      "================================================================================================================\n",
      "[device] cuda:2 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cbefb35d01177284\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-cbefb35d01177284/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e06fe2cef524c018e89aaebb0f07f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[tokenizer(ElectraTokenizerFast)] PreTrainedTokenizerFast(name_or_path='monologg/koelectra-base-v3-discriminator', vocab_size=35000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '한국어', '사전', '##학습', '모델', '##을', '공유', '##합니다', '.', '[SEP]']\n",
      "- ids    = [2, 11229, 7485, 26694, 6918, 4292, 7824, 17788, 18, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666a695d9b424a3293705496e4db9227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= [CLS] 숙소 위치 ##는 찾기 쉽 ##고 일반 ##적 ##인 한국 ##의 반지 ##하 숙소 ##입니다 . [SEP] 숙박 ##시설 ##의 위치 ##는 쉽 ##게 ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ec57fb5c7144c59aec03302458fdd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] ElectraModel(\n",
      "  (embeddings): ElectraEmbeddings(\n",
      "    (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      ")\n",
      "-      input_ids(2x512) : [2, 11229, 7485, 26694, 6918, 4292, 7824, 17788, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[ 0.0427, -0.5928, -0.3165,  ..., -0.1792, -0.4459, -0.3938],\n",
      "        [ 0.7741,  0.0279, -0.5623,  ...,  0.1221,  0.2013, -0.2910],\n",
      "        [ 0.4709,  0.1436, -0.6242,  ..., -0.2247,  0.0324, -0.5515],\n",
      "        ...,\n",
      "        [-0.1424, -0.7418, -0.3524,  ..., -0.0778,  0.1921,  0.2488],\n",
      "        [-0.0301, -0.6212, -0.2604,  ...,  0.0102,  0.1351,  0.0730],\n",
      "        [ 0.0427, -0.5928, -0.3165,  ..., -0.1792, -0.4459, -0.3938]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] pearson, spearmanr\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.024064 | Train pearson=0.9848, spearmanr=0.9524 | Valid pearson=0.9210, spearmanr=0.9165\n",
      "Epoch 02 | Loss 0.015111 | Train pearson=0.9898, spearmanr=0.9657 | Valid pearson=0.9180, spearmanr=0.9131\n",
      "Epoch 03 | Loss 0.012599 | Train pearson=0.9915, spearmanr=0.9735 | Valid pearson=0.9175, spearmanr=0.9170\n",
      "Epoch 04 | Loss 0.011414 | Train pearson=0.9934, spearmanr=0.9779 | Valid pearson=0.9144, spearmanr=0.9065\n",
      "Epoch 05 | Loss 0.010401 | Train pearson=0.9935, spearmanr=0.9796 | Valid pearson=0.9148, spearmanr=0.9157\n",
      "Epoch 06 | Loss 0.009794 | Train pearson=0.9950, spearmanr=0.9831 | Valid pearson=0.9259, spearmanr=0.9192\n",
      "Epoch 07 | Loss 0.009125 | Train pearson=0.9955, spearmanr=0.9836 | Valid pearson=0.9265, spearmanr=0.9225\n",
      "Epoch 08 | Loss 0.008501 | Train pearson=0.9960, spearmanr=0.9844 | Valid pearson=0.9268, spearmanr=0.9205\n",
      "Epoch 09 | Loss 0.008361 | Train pearson=0.9956, spearmanr=0.9849 | Valid pearson=0.9290, spearmanr=0.9198\n",
      "Epoch 10 | Loss 0.008071 | Train pearson=0.9964, spearmanr=0.9865 | Valid pearson=0.9275, spearmanr=0.9184\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-01-05 02:02:55] [END] TrainerForRegression.define().train()\n"
     ]
    }
   ],
   "source": [
    "!date +\"[%Y-%m-%d %H:%I:%S] [BEGIN] TrainerForRegression.define().train()\"\n",
    "TrainerForRegression(gpu_id=eid, lang_model=lang_models[eid], max_epoch=10,\n",
    "                     learning_rate=1e-5, max_seq_length=512).define().train()\n",
    "!date +\"[%Y-%m-%d %H:%I:%S] [END] TrainerForRegression.define().train()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### learning_rate=2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-01-05 02:02:55] [BEGIN] TrainerForRegression.define().train()\n",
      "\n",
      "================================================================================================================\n",
      "[device] cuda:2 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cbefb35d01177284\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-cbefb35d01177284/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7738b4b43b504788894107cbbb4a72c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[tokenizer(ElectraTokenizerFast)] PreTrainedTokenizerFast(name_or_path='monologg/koelectra-base-v3-discriminator', vocab_size=35000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '한국어', '사전', '##학습', '모델', '##을', '공유', '##합니다', '.', '[SEP]']\n",
      "- ids    = [2, 11229, 7485, 26694, 6918, 4292, 7824, 17788, 18, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51333e1a539b4bb6aee6e0cc2491a485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= [CLS] 숙소 위치 ##는 찾기 쉽 ##고 일반 ##적 ##인 한국 ##의 반지 ##하 숙소 ##입니다 . [SEP] 숙박 ##시설 ##의 위치 ##는 쉽 ##게 ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91e33eb320b4718987ae35f7da58313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] ElectraModel(\n",
      "  (embeddings): ElectraEmbeddings(\n",
      "    (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      ")\n",
      "-      input_ids(2x512) : [2, 11229, 7485, 26694, 6918, 4292, 7824, 17788, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[ 0.0427, -0.5928, -0.3165,  ..., -0.1792, -0.4459, -0.3938],\n",
      "        [ 0.7741,  0.0279, -0.5623,  ...,  0.1221,  0.2013, -0.2910],\n",
      "        [ 0.4709,  0.1436, -0.6242,  ..., -0.2247,  0.0324, -0.5515],\n",
      "        ...,\n",
      "        [-0.1424, -0.7418, -0.3524,  ..., -0.0778,  0.1921,  0.2488],\n",
      "        [-0.0301, -0.6212, -0.2604,  ...,  0.0102,  0.1351,  0.0730],\n",
      "        [ 0.0427, -0.5928, -0.3165,  ..., -0.1792, -0.4459, -0.3938]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] pearson, spearmanr\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.023042 | Train pearson=0.9842, spearmanr=0.9542 | Valid pearson=0.9179, spearmanr=0.9109\n",
      "Epoch 02 | Loss 0.015679 | Train pearson=0.9867, spearmanr=0.9650 | Valid pearson=0.9119, spearmanr=0.9213\n",
      "Epoch 03 | Loss 0.013114 | Train pearson=0.9907, spearmanr=0.9728 | Valid pearson=0.9282, spearmanr=0.9197\n",
      "Epoch 04 | Loss 0.011999 | Train pearson=0.9907, spearmanr=0.9761 | Valid pearson=0.9074, spearmanr=0.9133\n",
      "Epoch 05 | Loss 0.011425 | Train pearson=0.9933, spearmanr=0.9795 | Valid pearson=0.9201, spearmanr=0.9155\n",
      "Epoch 06 | Loss 0.010464 | Train pearson=0.9941, spearmanr=0.9818 | Valid pearson=0.9190, spearmanr=0.9120\n",
      "Epoch 07 | Loss 0.009760 | Train pearson=0.9948, spearmanr=0.9823 | Valid pearson=0.9234, spearmanr=0.9195\n",
      "Epoch 08 | Loss 0.009310 | Train pearson=0.9945, spearmanr=0.9825 | Valid pearson=0.9148, spearmanr=0.9168\n",
      "Epoch 09 | Loss 0.008913 | Train pearson=0.9952, spearmanr=0.9835 | Valid pearson=0.9246, spearmanr=0.9194\n",
      "Epoch 10 | Loss 0.008802 | Train pearson=0.9951, spearmanr=0.9845 | Valid pearson=0.9286, spearmanr=0.9211\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-01-05 04:04:30] [END] TrainerForRegression.define().train()\n"
     ]
    }
   ],
   "source": [
    "!date +\"[%Y-%m-%d %H:%I:%S] [BEGIN] TrainerForRegression.define().train()\"\n",
    "TrainerForRegression(gpu_id=eid, lang_model=lang_models[eid], max_epoch=10,\n",
    "                     learning_rate=2e-5, max_seq_length=512).define().train()\n",
    "!date +\"[%Y-%m-%d %H:%I:%S] [END] TrainerForRegression.define().train()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classification using KoELECTRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning_rate=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-01-05 04:04:30] [BEGIN] TrainerForClassification.define().train()\n",
      "\n",
      "================================================================================================================\n",
      "[device] cuda:2 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b3f46ff75b254c98\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-b3f46ff75b254c98/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a43261591dd4b67ba00cf2b38b7bdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[tokenizer(ElectraTokenizerFast)] PreTrainedTokenizerFast(name_or_path='monologg/koelectra-base-v3-discriminator', vocab_size=35000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '한국어', '사전', '##학습', '모델', '##을', '공유', '##합니다', '.', '[SEP]']\n",
      "- ids    = [2, 11229, 7485, 26694, 6918, 4292, 7824, 17788, 18, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d14d7457cbd4a4f9782c2e1eecd4a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= [CLS] 숙소 위치 ##는 찾기 쉽 ##고 일반 ##적 ##인 한국 ##의 반지 ##하 숙소 ##입니다 . [SEP] 숙박 ##시설 ##의 위치 ##는 쉽 ##게 ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f75a9fed6284935813af39c52f8d557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] ElectraModel(\n",
      "  (embeddings): ElectraEmbeddings(\n",
      "    (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      ")\n",
      "-      input_ids(2x512) : [2, 11229, 7485, 26694, 6918, 4292, 7824, 17788, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[ 0.0427, -0.5928, -0.3165,  ..., -0.1792, -0.4459, -0.3938],\n",
      "        [ 0.7741,  0.0279, -0.5623,  ...,  0.1221,  0.2013, -0.2910],\n",
      "        [ 0.4709,  0.1436, -0.6242,  ..., -0.2247,  0.0324, -0.5515],\n",
      "        ...,\n",
      "        [-0.1424, -0.7418, -0.3524,  ..., -0.0778,  0.1921,  0.2488],\n",
      "        [-0.0301, -0.6212, -0.2604,  ...,  0.0102,  0.1351,  0.0730],\n",
      "        [ 0.0427, -0.5928, -0.3165,  ..., -0.1792, -0.4459, -0.3938]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] accuracy, f1\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.154008 | Train accuracy=0.9627, f1=0.9620 | Valid accuracy=0.8208, f1=0.8158\n",
      "Epoch 02 | Loss 0.109398 | Train accuracy=0.9649, f1=0.9644 | Valid accuracy=0.7938, f1=0.7977\n",
      "Epoch 03 | Loss 0.090882 | Train accuracy=0.9668, f1=0.9652 | Valid accuracy=0.8304, f1=0.8044\n",
      "Epoch 04 | Loss 0.083904 | Train accuracy=0.9721, f1=0.9714 | Valid accuracy=0.8362, f1=0.8276\n",
      "Epoch 05 | Loss 0.084255 | Train accuracy=0.9697, f1=0.9691 | Valid accuracy=0.7977, f1=0.7985\n",
      "Epoch 06 | Loss 0.083106 | Train accuracy=0.9753, f1=0.9743 | Valid accuracy=0.8343, f1=0.8178\n",
      "Epoch 07 | Loss 0.072984 | Train accuracy=0.9647, f1=0.9625 | Valid accuracy=0.7803, f1=0.7246\n",
      "Epoch 08 | Loss 0.074540 | Train accuracy=0.9776, f1=0.9768 | Valid accuracy=0.8285, f1=0.8142\n",
      "Epoch 09 | Loss 0.070985 | Train accuracy=0.9677, f1=0.9657 | Valid accuracy=0.8015, f1=0.7542\n",
      "Epoch 10 | Loss 0.058943 | Train accuracy=0.9739, f1=0.9726 | Valid accuracy=0.8324, f1=0.8071\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-01-05 05:05:37] [END] TrainerForClassification.define().train()\n"
     ]
    }
   ],
   "source": [
    "!date +\"[%Y-%m-%d %H:%I:%S] [BEGIN] TrainerForClassification.define().train()\"\n",
    "TrainerForClassification(gpu_id=eid, lang_model=lang_models[eid], max_epoch=10,\n",
    "                         learning_rate=1e-5, max_seq_length=512).define().train()\n",
    "!date +\"[%Y-%m-%d %H:%I:%S] [END] TrainerForClassification.define().train()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning_rate=2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-01-05 05:05:38] [BEGIN] TrainerForClassification.define().train()\n",
      "\n",
      "================================================================================================================\n",
      "[device] cuda:2 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b3f46ff75b254c98\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-b3f46ff75b254c98/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10ade8d3512468da284d2b0c2f5660e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[tokenizer(ElectraTokenizerFast)] PreTrainedTokenizerFast(name_or_path='monologg/koelectra-base-v3-discriminator', vocab_size=35000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '한국어', '사전', '##학습', '모델', '##을', '공유', '##합니다', '.', '[SEP]']\n",
      "- ids    = [2, 11229, 7485, 26694, 6918, 4292, 7824, 17788, 18, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ba2ca8ae5c49ea9f57e920aa1be02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= [CLS] 숙소 위치 ##는 찾기 쉽 ##고 일반 ##적 ##인 한국 ##의 반지 ##하 숙소 ##입니다 . [SEP] 숙박 ##시설 ##의 위치 ##는 쉽 ##게 ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5012af02994f4cd4836a2eea6804b8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] ElectraModel(\n",
      "  (embeddings): ElectraEmbeddings(\n",
      "    (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      ")\n",
      "-      input_ids(2x512) : [2, 11229, 7485, 26694, 6918, 4292, 7824, 17788, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[ 0.0427, -0.5928, -0.3165,  ..., -0.1792, -0.4459, -0.3938],\n",
      "        [ 0.7741,  0.0279, -0.5623,  ...,  0.1221,  0.2013, -0.2910],\n",
      "        [ 0.4709,  0.1436, -0.6242,  ..., -0.2247,  0.0324, -0.5515],\n",
      "        ...,\n",
      "        [-0.1424, -0.7418, -0.3524,  ..., -0.0778,  0.1921,  0.2488],\n",
      "        [-0.0301, -0.6212, -0.2604,  ...,  0.0102,  0.1351,  0.0730],\n",
      "        [ 0.0427, -0.5928, -0.3165,  ..., -0.1792, -0.4459, -0.3938]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] accuracy, f1\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.162288 | Train accuracy=0.8892, f1=0.8710 | Valid accuracy=0.7495, f1=0.6221\n",
      "Epoch 02 | Loss 0.134914 | Train accuracy=0.9515, f1=0.9509 | Valid accuracy=0.7861, f1=0.7828\n",
      "Epoch 03 | Loss 0.131196 | Train accuracy=0.9524, f1=0.9518 | Valid accuracy=0.8150, f1=0.8065\n",
      "Epoch 04 | Loss 0.142574 | Train accuracy=0.9639, f1=0.9627 | Valid accuracy=0.8131, f1=0.7966\n",
      "Epoch 05 | Loss 0.100994 | Train accuracy=0.9625, f1=0.9618 | Valid accuracy=0.7977, f1=0.7904\n",
      "Epoch 06 | Loss 0.142342 | Train accuracy=0.9673, f1=0.9664 | Valid accuracy=0.8382, f1=0.8286\n",
      "Epoch 07 | Loss 0.100272 | Train accuracy=0.9695, f1=0.9682 | Valid accuracy=0.8593, f1=0.8437\n",
      "Epoch 08 | Loss 0.087440 | Train accuracy=0.9655, f1=0.9638 | Valid accuracy=0.8266, f1=0.7907\n",
      "Epoch 09 | Loss 0.118976 | Train accuracy=0.9438, f1=0.9390 | Valid accuracy=0.7707, f1=0.6957\n",
      "Epoch 10 | Loss 0.137924 | Train accuracy=0.9546, f1=0.9521 | Valid accuracy=0.7958, f1=0.7558\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-01-05 07:07:04] [END] TrainerForClassification.define().train()\n"
     ]
    }
   ],
   "source": [
    "!date +\"[%Y-%m-%d %H:%I:%S] [BEGIN] TrainerForClassification.define().train()\"\n",
    "TrainerForClassification(gpu_id=eid, lang_model=lang_models[eid], max_epoch=10,\n",
    "                         learning_rate=2e-5, max_seq_length=512).define().train()\n",
    "!date +\"[%Y-%m-%d %H:%I:%S] [END] TrainerForClassification.define().train()\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter LTN",
   "language": "python",
   "name": "ltn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
