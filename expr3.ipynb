{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Python path: /home/chris/miniconda3/envs/LTN/bin/python\n",
      "* Library version:\n",
      "  - numpy 1.21.5\r\n",
      "  - datasets 1.17.0\r\n",
      "  - torch 1.10.1\r\n",
      "  - LTNtorch 0.9\r\n",
      "  - transformers 4.15.0\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "python = sys.executable\n",
    "print(f\"* Python path: {python}\")\n",
    "print(f\"* Library version:\")\n",
    "!echo '  -' `$python -m pip list | grep -w numpy`\n",
    "!echo '  -' `$python -m pip list | grep -w datasets`\n",
    "!echo '  -' `$python -m pip list | grep -w torch`\n",
    "!echo '  -' `$python -m pip list | grep -w LTNtorch`\n",
    "!echo '  -' `$python -m pip list | grep -w transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* data_files:\n",
      "  - train: data/klue-sts-cls/train.json\n",
      "  - valid: data/klue-sts-cls/valid.json\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "total 3764\r\n",
      "-rw-rw-r-- 1 chris chris 3690197 Dec 30 16:55 train.json\r\n",
      "-rw-rw-r-- 1 chris chris  160929 Dec 30 16:55 valid.json\r\n"
     ]
    }
   ],
   "source": [
    "from main import data_files\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(data_files['train']).parent\n",
    "\n",
    "print(f\"* data_files:\")\n",
    "for k, v in data_files.items():\n",
    "    print(f\"  - {k}: {v}\")\n",
    "print('-' * 112)\n",
    "!ls -l $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* available gpu_ids: (0, 1, 2, 3)\n",
      "* gpu_id: 3\n",
      "* available lang_models: \n",
      "  - bert-base-multilingual-uncased\n",
      "  - skt/kobert-base-v1\n",
      "  - monologg/koelectra-base-v3-discriminator\n",
      "  - monologg/kobigbird-bert-base\n",
      "* lang_model: monologg/kobigbird-bert-base\n"
     ]
    }
   ],
   "source": [
    "from main import gpu_ids, lang_models, do_experiment\n",
    "\n",
    "gpu_id = 3\n",
    "print(f\"* available gpu_ids: {gpu_ids}\")\n",
    "print(f\"* gpu_id: {gpu_id}\")\n",
    "\n",
    "lm_id = 3\n",
    "print(f\"* available lang_models: \")\n",
    "for m in lang_models:\n",
    "    print(f\"  - {m}\")\n",
    "print(f\"* lang_model: {lang_models[lm_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KLUE-STS(cls) [KoBigBird]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[device] cuda:3 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b3f46ff75b254c98\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-b3f46ff75b254c98/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5dccfbf73ec4f81a48e925ccc57a5e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[tokenizer(BertTokenizerFast)] PreTrainedTokenizerFast(name_or_path='monologg/kobigbird-bert-base', vocab_size=32500, model_max_len=4096, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '한국어', '사전', '##학습', '모델', '##을', '공유', '##합니다', '.', '[SEP]']\n",
      "- ids    = [2, 11802, 8089, 26302, 7498, 4604, 8398, 15763, 518, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c739f50781154074aaa87aa80d60f126"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= ['[CLS]', '숙소', '위치', '##는', '찾', '##기', '쉽', '##고', '일반', '##적', '##인', '한국', '##의', '반지', '##하', '숙소', '##입니다', '.', '[SEP]', '숙박', '##시설', '##의', '위치', '##는', '쉽', '...', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed6d12daa6044e23a3340660629243cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BigBirdModel(\n",
      "  (embeddings): BigBirdEmbeddings(\n",
      "    (word_embeddings): Embedding(32500, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(4096, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "-      input_ids(2x512) : [2, 11802, 8089, 26302, 7498, 4604, 8398, 15763, 518, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[-0.0115,  0.0176, -0.5810,  ..., -0.0717, -0.2153,  0.1399],\n",
      "        [-0.1844,  0.0906, -0.2161,  ..., -0.0128,  0.1295, -0.1789],\n",
      "        [ 0.0818, -0.0373, -0.4283,  ..., -0.0713,  0.3130,  0.6987],\n",
      "        ...,\n",
      "        [-0.2158, -0.1442, -0.0021,  ..., -0.2636,  0.1824,  0.3736],\n",
      "        [ 0.0559, -0.0611, -0.0294,  ..., -0.1618,  0.2605,  0.6560],\n",
      "        [ 0.0663, -0.0542, -0.0460,  ..., -0.1770,  0.1802,  0.5826]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] glue/mrpc\n",
      "- GLUE, the General Language Understanding Evaluation benchmark\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.272683 | Train acc=0.8624, f1=0.8370 | Valid acc=0.7514, f1=0.6446\n",
      "Epoch 02 | Loss 0.316343 | Train acc=0.8871, f1=0.8772 | Valid acc=0.6898, f1=0.6611\n",
      "Epoch 03 | Loss 0.203013 | Train acc=0.9255, f1=0.9199 | Valid acc=0.8304, f1=0.7934\n",
      "Epoch 04 | Loss 0.316609 | Train acc=0.5423, f1=0.1526 | Valid acc=0.5934, f1=0.1388\n",
      "Epoch 05 | Loss 0.420269 | Train acc=0.8382, f1=0.8266 | Valid acc=0.5530, f1=0.5304\n",
      "Epoch 06 | Loss 0.328644 | Train acc=0.8162, f1=0.8392 | Valid acc=0.5742, f1=0.6636\n",
      "Epoch 07 | Loss 0.260735 | Train acc=0.9276, f1=0.9275 | Valid acc=0.7437, f1=0.7476\n",
      "Epoch 08 | Loss 0.309068 | Train acc=0.9188, f1=0.9163 | Valid acc=0.7572, f1=0.7284\n",
      "Epoch 09 | Loss 0.279163 | Train acc=0.4864, f1=0.6515 | Valid acc=0.4258, f1=0.5962\n",
      "Epoch 10 | Loss 0.364300 | Train acc=0.8798, f1=0.8745 | Valid acc=0.7514, f1=0.7273\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_experiment(gpu_id=gpu_id, lang_model=lang_models[lm_id], learning_rate=2e-5, max_seq_length=512, max_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[device] cuda:3 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b3f46ff75b254c98\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-b3f46ff75b254c98/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "393425c8222d419e9b648f99e0aff31a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[tokenizer(BertTokenizerFast)] PreTrainedTokenizerFast(name_or_path='monologg/kobigbird-bert-base', vocab_size=32500, model_max_len=4096, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '한국어', '사전', '##학습', '모델', '##을', '공유', '##합니다', '.', '[SEP]']\n",
      "- ids    = [2, 11802, 8089, 26302, 7498, 4604, 8398, 15763, 518, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3468a5d5172d432dafc1ad81fc9fdadf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= ['[CLS]', '숙소', '위치', '##는', '찾', '##기', '쉽', '##고', '일반', '##적', '##인', '한국', '##의', '반지', '##하', '숙소', '##입니다', '.', '[SEP]', '숙박', '##시설', '##의', '위치', '##는', '쉽', '...', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "344cc376212943209c551803b94427ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BigBirdModel(\n",
      "  (embeddings): BigBirdEmbeddings(\n",
      "    (word_embeddings): Embedding(32500, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(4096, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "-      input_ids(2x512) : [2, 11802, 8089, 26302, 7498, 4604, 8398, 15763, 518, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[-0.0115,  0.0176, -0.5810,  ..., -0.0717, -0.2153,  0.1399],\n",
      "        [-0.1844,  0.0906, -0.2161,  ..., -0.0128,  0.1295, -0.1789],\n",
      "        [ 0.0818, -0.0373, -0.4283,  ..., -0.0713,  0.3130,  0.6987],\n",
      "        ...,\n",
      "        [-0.2158, -0.1442, -0.0021,  ..., -0.2636,  0.1824,  0.3736],\n",
      "        [ 0.0559, -0.0611, -0.0294,  ..., -0.1618,  0.2605,  0.6560],\n",
      "        [ 0.0663, -0.0542, -0.0460,  ..., -0.1770,  0.1802,  0.5826]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] glue/mrpc\n",
      "- GLUE, the General Language Understanding Evaluation benchmark\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.216605 | Train acc=0.9502, f1=0.9482 | Valid acc=0.7592, f1=0.7265\n",
      "Epoch 02 | Loss 0.160273 | Train acc=0.8948, f1=0.9011 | Valid acc=0.6802, f1=0.7252\n",
      "Epoch 03 | Loss 0.179547 | Train acc=0.9128, f1=0.9164 | Valid acc=0.6975, f1=0.7352\n",
      "Epoch 04 | Loss 0.169412 | Train acc=0.9261, f1=0.9279 | Valid acc=0.7399, f1=0.7576\n",
      "Epoch 05 | Loss 0.188336 | Train acc=0.9272, f1=0.9223 | Valid acc=0.7457, f1=0.7215\n",
      "Epoch 06 | Loss 0.187735 | Train acc=0.9213, f1=0.9133 | Valid acc=0.7996, f1=0.7374\n",
      "Epoch 07 | Loss 0.271274 | Train acc=0.9230, f1=0.9159 | Valid acc=0.7572, f1=0.6897\n",
      "Epoch 08 | Loss 0.133361 | Train acc=0.9638, f1=0.9629 | Valid acc=0.8170, f1=0.8081\n",
      "Epoch 09 | Loss 0.147383 | Train acc=0.9524, f1=0.9506 | Valid acc=0.8401, f1=0.8215\n",
      "Epoch 10 | Loss 0.123810 | Train acc=0.9206, f1=0.9234 | Valid acc=0.7168, f1=0.7487\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_experiment(gpu_id=gpu_id, lang_model=lang_models[lm_id], learning_rate=1e-5, max_seq_length=512, max_epoch=10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter LTN",
   "language": "python",
   "name": "ltn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}