{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Python Path: /home/chris/miniconda3/envs/LTN/bin/python\n",
      "* Library Version:\n",
      "  - torch 1.10.1\n",
      "  - LTNtorch 0.9\n",
      "  - transformers 4.15.0\n",
      "* Experiment ID: 3\n",
      "* Pretrained Model: monologg/kobigbird-bert-base\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from main import lang_models, TrainerForClassification, TrainerForRegression\n",
    "\n",
    "eid = 3\n",
    "python = sys.executable\n",
    "print(f\"* Python Path: {python}\")\n",
    "print(f\"* Library Version:\")\n",
    "!echo '  -' `$python -m pip list | grep -w torch`\n",
    "!echo '  -' `$python -m pip list | grep -w LTNtorch`\n",
    "!echo '  -' `$python -m pip list | grep -w transformers`\n",
    "print(f\"* Experiment ID: {eid}\")\n",
    "print(f\"* Pretrained Model: {lang_models[eid]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression using KoBigBird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_seq_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-05 00:12:10] [BEGIN] TrainerForRegression.define().train()\n",
      "\n",
      "================================================================================================================\n",
      "[device] cuda:3 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cbefb35d01177284\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-cbefb35d01177284/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea535d37d98b459d80c83834341844ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[tokenizer(BertTokenizerFast)] PreTrainedTokenizerFast(name_or_path='monologg/kobigbird-bert-base', vocab_size=32500, model_max_len=4096, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '한국어', '사전', '##학습', '모델', '##을', '공유', '##합니다', '.', '[SEP]']\n",
      "- ids    = [2, 11802, 8089, 26302, 7498, 4604, 8398, 15763, 518, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e03e5ec51540e1b79b9a09a87b7e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= [CLS] 숙소 위치 ##는 찾 ##기 쉽 ##고 일반 ##적 ##인 한국 ##의 반지 ##하 숙소 ##입니다 . [SEP] 숙박 ##시설 ##의 위치 ##는 쉽 ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9353f6b9a5b454682040aec361ee3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BigBirdModel(\n",
      "  (embeddings): BigBirdEmbeddings(\n",
      "    (word_embeddings): Embedding(32500, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(4096, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "-      input_ids(2x512) : [2, 11802, 8089, 26302, 7498, 4604, 8398, 15763, 518, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[-0.0115,  0.0176, -0.5810,  ..., -0.0717, -0.2153,  0.1399],\n",
      "        [-0.1844,  0.0906, -0.2161,  ..., -0.0128,  0.1295, -0.1789],\n",
      "        [ 0.0818, -0.0373, -0.4283,  ..., -0.0713,  0.3130,  0.6987],\n",
      "        ...,\n",
      "        [-0.2158, -0.1442, -0.0021,  ..., -0.2636,  0.1824,  0.3736],\n",
      "        [ 0.0559, -0.0611, -0.0294,  ..., -0.1618,  0.2605,  0.6560],\n",
      "        [ 0.0663, -0.0542, -0.0460,  ..., -0.1770,  0.1802,  0.5826]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] pearson, spearmanr\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.025780 | Train pearson=0.9819, spearmanr=0.9459 | Valid pearson=0.8943, spearmanr=0.8974\n",
      "Epoch 02 | Loss 0.016668 | Train pearson=0.9853, spearmanr=0.9609 | Valid pearson=0.8926, spearmanr=0.8989\n",
      "Epoch 03 | Loss 0.014327 | Train pearson=0.9901, spearmanr=0.9691 | Valid pearson=0.8989, spearmanr=0.9007\n",
      "Epoch 04 | Loss 0.012898 | Train pearson=0.9922, spearmanr=0.9723 | Valid pearson=0.9125, spearmanr=0.9094\n",
      "Epoch 05 | Loss 0.011521 | Train pearson=0.9929, spearmanr=0.9767 | Valid pearson=0.9109, spearmanr=0.9182\n",
      "Epoch 06 | Loss 0.010326 | Train pearson=0.9934, spearmanr=0.9777 | Valid pearson=0.9114, spearmanr=0.9128\n",
      "Epoch 07 | Loss 0.010001 | Train pearson=0.9946, spearmanr=0.9814 | Valid pearson=0.9096, spearmanr=0.9137\n",
      "Epoch 08 | Loss 0.009260 | Train pearson=0.9948, spearmanr=0.9824 | Valid pearson=0.9116, spearmanr=0.9069\n",
      "Epoch 09 | Loss 0.009079 | Train pearson=0.9945, spearmanr=0.9817 | Valid pearson=0.9015, spearmanr=0.9118\n",
      "Epoch 10 | Loss 0.008939 | Train pearson=0.9955, spearmanr=0.9833 | Valid pearson=0.9154, spearmanr=0.9146\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-01-05 01:01:17] [END] TrainerForRegression.define().train()\n"
     ]
    }
   ],
   "source": [
    "!date +\"[%Y-%m-%d %H:%I:%S] [BEGIN] TrainerForRegression.define().train()\"\n",
    "TrainerForRegression(gpu_id=eid, lang_model=lang_models[eid], max_epoch=10,\n",
    "                     learning_rate=1e-5, max_seq_length=512).define().train()\n",
    "!date +\"[%Y-%m-%d %H:%I:%S] [END] TrainerForRegression.define().train()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_seq_length=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-01-05 01:01:17] [BEGIN] TrainerForRegression.define().train()\n",
      "\n",
      "================================================================================================================\n",
      "[device] cuda:3 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cbefb35d01177284\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-cbefb35d01177284/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7110cd453cf84b4daa2a523cdbbd0ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[tokenizer(BertTokenizerFast)] PreTrainedTokenizerFast(name_or_path='monologg/kobigbird-bert-base', vocab_size=32500, model_max_len=4096, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '한국어', '사전', '##학습', '모델', '##을', '공유', '##합니다', '.', '[SEP]']\n",
      "- ids    = [2, 11802, 8089, 26302, 7498, 4604, 8398, 15763, 518, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a71d5cb77949efbb1fc024558c3144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](1024)\t= [CLS] 숙소 위치 ##는 찾 ##기 쉽 ##고 일반 ##적 ##인 한국 ##의 반지 ##하 숙소 ##입니다 . [SEP] 숙박 ##시설 ##의 위치 ##는 쉽 ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02f426785b147c7b856f617f4ccbd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/chris/miniconda3/envs/LTN/lib/python3.8/site-packages/transformers/models/big_bird/modeling_big_bird.py:976: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BigBirdModel(\n",
      "  (embeddings): BigBirdEmbeddings(\n",
      "    (word_embeddings): Embedding(32500, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(4096, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "-      input_ids(2x1024) : [2, 11802, 8089, 26302, 7498, 4604, 8398, 15763, 518, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- attention_mask(2x1024) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x1024) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x1024x768) : tensor([[-0.0115,  0.0176, -0.5810,  ..., -0.0717, -0.2153,  0.1399],\n",
      "        [-0.1844,  0.0906, -0.2161,  ..., -0.0128,  0.1295, -0.1789],\n",
      "        [ 0.0818, -0.0373, -0.4283,  ..., -0.0713,  0.3130,  0.6987],\n",
      "        ...,\n",
      "        [ 0.2074,  0.4346, -0.2698,  ...,  0.0503, -0.3272, -0.1166],\n",
      "        [ 0.2212,  0.4370, -0.2720,  ...,  0.0472, -0.3289, -0.1206],\n",
      "        [ 0.2168,  0.4393, -0.2702,  ...,  0.0507, -0.3230, -0.1166]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] pearson, spearmanr\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.026193 | Train pearson=0.9824, spearmanr=0.9457 | Valid pearson=0.9004, spearmanr=0.8990\n",
      "Epoch 02 | Loss 0.016515 | Train pearson=0.9849, spearmanr=0.9599 | Valid pearson=0.8923, spearmanr=0.9022\n",
      "Epoch 03 | Loss 0.014188 | Train pearson=0.9900, spearmanr=0.9690 | Valid pearson=0.9085, spearmanr=0.9033\n",
      "Epoch 04 | Loss 0.012392 | Train pearson=0.9918, spearmanr=0.9741 | Valid pearson=0.9120, spearmanr=0.9061\n",
      "Epoch 05 | Loss 0.011431 | Train pearson=0.9925, spearmanr=0.9760 | Valid pearson=0.9065, spearmanr=0.9139\n",
      "Epoch 06 | Loss 0.010693 | Train pearson=0.9943, spearmanr=0.9791 | Valid pearson=0.9120, spearmanr=0.9129\n",
      "Epoch 07 | Loss 0.010011 | Train pearson=0.9949, spearmanr=0.9800 | Valid pearson=0.9152, spearmanr=0.9123\n",
      "Epoch 08 | Loss 0.009512 | Train pearson=0.9949, spearmanr=0.9823 | Valid pearson=0.9107, spearmanr=0.9162\n",
      "Epoch 09 | Loss 0.009106 | Train pearson=0.9947, spearmanr=0.9825 | Valid pearson=0.9086, spearmanr=0.9180\n",
      "Epoch 10 | Loss 0.008583 | Train pearson=0.9955, spearmanr=0.9823 | Valid pearson=0.9145, spearmanr=0.9094\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-01-05 07:07:57] [END] TrainerForRegression.define().train()\n"
     ]
    }
   ],
   "source": [
    "!date +\"[%Y-%m-%d %H:%I:%S] [BEGIN] TrainerForRegression.define().train()\"\n",
    "TrainerForRegression(gpu_id=eid, lang_model=lang_models[eid], max_epoch=10,\n",
    "                     learning_rate=1e-5, max_seq_length=1024).define().train()\n",
    "!date +\"[%Y-%m-%d %H:%I:%S] [END] TrainerForRegression.define().train()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using KoBigBird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_seq_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-01-05 07:07:57] [BEGIN] TrainerForClassification.define().train()\n",
      "\n",
      "================================================================================================================\n",
      "[device] cuda:3 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b3f46ff75b254c98\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-b3f46ff75b254c98/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7587699c03e04eb68b39a686a5474f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[tokenizer(BertTokenizerFast)] PreTrainedTokenizerFast(name_or_path='monologg/kobigbird-bert-base', vocab_size=32500, model_max_len=4096, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '한국어', '사전', '##학습', '모델', '##을', '공유', '##합니다', '.', '[SEP]']\n",
      "- ids    = [2, 11802, 8089, 26302, 7498, 4604, 8398, 15763, 518, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4539701fb04c3985ab7dc60d4b4665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= [CLS] 숙소 위치 ##는 찾 ##기 쉽 ##고 일반 ##적 ##인 한국 ##의 반지 ##하 숙소 ##입니다 . [SEP] 숙박 ##시설 ##의 위치 ##는 쉽 ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d1579ec419464da7ae666be8c4a194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BigBirdModel(\n",
      "  (embeddings): BigBirdEmbeddings(\n",
      "    (word_embeddings): Embedding(32500, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(4096, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "-      input_ids(2x512) : [2, 11802, 8089, 26302, 7498, 4604, 8398, 15763, 518, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[-0.0115,  0.0176, -0.5810,  ..., -0.0717, -0.2153,  0.1399],\n",
      "        [-0.1844,  0.0906, -0.2161,  ..., -0.0128,  0.1295, -0.1789],\n",
      "        [ 0.0818, -0.0373, -0.4283,  ..., -0.0713,  0.3130,  0.6987],\n",
      "        ...,\n",
      "        [-0.2158, -0.1442, -0.0021,  ..., -0.2636,  0.1824,  0.3736],\n",
      "        [ 0.0559, -0.0611, -0.0294,  ..., -0.1618,  0.2605,  0.6560],\n",
      "        [ 0.0663, -0.0542, -0.0460,  ..., -0.1770,  0.1802,  0.5826]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] accuracy, f1\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.175266 | Train accuracy=0.9591, f1=0.9574 | Valid accuracy=0.7842, f1=0.7533\n",
      "Epoch 02 | Loss 0.145207 | Train accuracy=0.9606, f1=0.9593 | Valid accuracy=0.7881, f1=0.7679\n",
      "Epoch 03 | Loss 0.134415 | Train accuracy=0.8798, f1=0.8596 | Valid accuracy=0.7457, f1=0.6250\n",
      "Epoch 04 | Loss 0.134332 | Train accuracy=0.9519, f1=0.9504 | Valid accuracy=0.8189, f1=0.8112\n"
     ]
    }
   ],
   "source": [
    "!date +\"[%Y-%m-%d %H:%I:%S] [BEGIN] TrainerForClassification.define().train()\"\n",
    "TrainerForClassification(gpu_id=eid, lang_model=lang_models[eid], max_epoch=10,\n",
    "                         learning_rate=1e-5, max_seq_length=512).define().train()\n",
    "!date +\"[%Y-%m-%d %H:%I:%S] [END] TrainerForClassification.define().train()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_seq_length=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!date +\"[%Y-%m-%d %H:%I:%S] [BEGIN] TrainerForClassification.define().train()\"\n",
    "TrainerForClassification(gpu_id=eid, lang_model=lang_models[eid], max_epoch=10,\n",
    "                         learning_rate=1e-5, max_seq_length=1024).define().train()\n",
    "!date +\"[%Y-%m-%d %H:%I:%S] [END] TrainerForClassification.define().train()\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter LTN",
   "language": "python",
   "name": "ltn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
