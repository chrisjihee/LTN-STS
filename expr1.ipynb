{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Python path: /home/chris/miniconda3/envs/LTN/bin/python\n",
      "* Library version:\n",
      "  - numpy 1.21.5\r\n",
      "  - datasets 1.17.0\r\n",
      "  - torch 1.10.1\r\n",
      "  - LTNtorch 0.9\r\n",
      "  - transformers 4.15.0\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "python = sys.executable\n",
    "print(f\"* Python path: {python}\")\n",
    "print(f\"* Library version:\")\n",
    "!echo '  -' `$python -m pip list | grep -w numpy`\n",
    "!echo '  -' `$python -m pip list | grep -w datasets`\n",
    "!echo '  -' `$python -m pip list | grep -w torch`\n",
    "!echo '  -' `$python -m pip list | grep -w LTNtorch`\n",
    "!echo '  -' `$python -m pip list | grep -w transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* data_files:\n",
      "  - train: data/klue-sts-cls/train.json\n",
      "  - valid: data/klue-sts-cls/valid.json\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "total 3764\r\n",
      "-rw-rw-r-- 1 chris chris 3690197 Dec 30 16:55 train.json\r\n",
      "-rw-rw-r-- 1 chris chris  160929 Dec 30 16:55 valid.json\r\n"
     ]
    }
   ],
   "source": [
    "from main import data_files\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(data_files['train']).parent\n",
    "\n",
    "print(f\"* data_files:\")\n",
    "for k, v in data_files.items():\n",
    "    print(f\"  - {k}: {v}\")\n",
    "print('-' * 112)\n",
    "!ls -l $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Env setting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* available gpu_ids: (0, 1, 2, 3)\n",
      "* gpu_id: 1\n",
      "* available lang_models: \n",
      "  - bert-base-multilingual-uncased\n",
      "  - skt/kobert-base-v1\n",
      "  - monologg/koelectra-base-v3-discriminator\n",
      "  - monologg/kobigbird-bert-base\n",
      "* lang_model: skt/kobert-base-v1\n"
     ]
    }
   ],
   "source": [
    "from main import gpu_ids, lang_models, do_experiment\n",
    "\n",
    "gpu_id = 1\n",
    "print(f\"* available gpu_ids: {gpu_ids}\")\n",
    "print(f\"* gpu_id: {gpu_id}\")\n",
    "\n",
    "lm_id = 1\n",
    "print(f\"* available lang_models: \")\n",
    "for m in lang_models:\n",
    "    print(f\"  - {m}\")\n",
    "print(f\"* lang_model: {lang_models[lm_id]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KLUE-STS(cls) [KoBERT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[device] cuda:1 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b3f46ff75b254c98\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-b3f46ff75b254c98/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bb7491151c14ba6898e9435f6205add"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[tokenizer(KoBERTTokenizer)] PreTrainedTokenizer(name_or_path='skt/kobert-base-v1', vocab_size=8002, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '▁한국', '어', '▁사전', '학습', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']\n",
      "- ids    = [2, 4958, 6855, 2625, 7826, 2046, 7088, 1050, 7843, 54, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2f0c62175c842e798ce465fec841f67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= ['[CLS]', '▁숙', '소', '▁위치', '는', '▁찾기', '▁', '쉽', '고', '▁일반', '적인', '▁한국의', '▁반', '지', '하', '▁숙', '소', '입니다', '.', '[SEP]', '▁숙', '박', '시설', '의', '▁위치', '...', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7782b506e5944883964ffdc3cbfa4575"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "-      input_ids(2x512) : [2, 4958, 6855, 2625, 7826, 2046, 7088, 1050, 7843, 54, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, '...', 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[-0.2184,  0.0321,  0.1199,  ..., -0.1616,  0.0468,  0.1257],\n",
      "        [ 0.0439, -0.3687, -0.0513,  ..., -0.6372, -0.1144, -0.3496],\n",
      "        [ 0.1696, -0.3503,  0.0459,  ..., -0.2876, -0.0355, -0.2889],\n",
      "        ...,\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237],\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237],\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] glue/mrpc\n",
      "- GLUE, the General Language Understanding Evaluation benchmark\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.180424 | Train acc=0.9358, f1=0.9367 | Valid acc=0.7437, f1=0.7621\n",
      "Epoch 02 | Loss 0.304681 | Train acc=0.7002, f1=0.6155 | Valid acc=0.6281, f1=0.4826\n",
      "Epoch 03 | Loss 0.310148 | Train acc=0.8225, f1=0.8431 | Valid acc=0.5723, f1=0.6616\n",
      "Epoch 04 | Loss 0.373076 | Train acc=0.5051, f1=0.5145 | Valid acc=0.5145, f1=0.5116\n",
      "Epoch 05 | Loss 0.331887 | Train acc=0.7266, f1=0.6057 | Valid acc=0.6146, f1=0.2647\n",
      "Epoch 06 | Loss 0.247857 | Train acc=0.8784, f1=0.8597 | Valid acc=0.7245, f1=0.6305\n",
      "Epoch 07 | Loss 0.288075 | Train acc=0.8539, f1=0.8669 | Valid acc=0.6320, f1=0.6914\n",
      "Epoch 08 | Loss 0.326474 | Train acc=0.4958, f1=0.5449 | Valid acc=0.4952, f1=0.5219\n",
      "Epoch 09 | Loss 0.484228 | Train acc=0.8029, f1=0.7757 | Valid acc=0.7052, f1=0.6531\n",
      "Epoch 10 | Loss 0.457211 | Train acc=0.5216, f1=0.4663 | Valid acc=0.5260, f1=0.4143\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_experiment(gpu_id=gpu_id, lang_model=lang_models[lm_id], learning_rate=2e-5, max_seq_length=512, max_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[device] cuda:1 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b3f46ff75b254c98\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-b3f46ff75b254c98/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93c15e15d83345fdadf376f2379d9bf7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[tokenizer(KoBERTTokenizer)] PreTrainedTokenizer(name_or_path='skt/kobert-base-v1', vocab_size=8002, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '▁한국', '어', '▁사전', '학습', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']\n",
      "- ids    = [2, 4958, 6855, 2625, 7826, 2046, 7088, 1050, 7843, 54, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad48c893f6bb484ebcc290d61051e95e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= ['[CLS]', '▁숙', '소', '▁위치', '는', '▁찾기', '▁', '쉽', '고', '▁일반', '적인', '▁한국의', '▁반', '지', '하', '▁숙', '소', '입니다', '.', '[SEP]', '▁숙', '박', '시설', '의', '▁위치', '...', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3478df6fbfe34176ae0a3fab4cf73965"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "-      input_ids(2x512) : [2, 4958, 6855, 2625, 7826, 2046, 7088, 1050, 7843, 54, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, '...', 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[-0.2184,  0.0321,  0.1199,  ..., -0.1616,  0.0468,  0.1257],\n",
      "        [ 0.0439, -0.3687, -0.0513,  ..., -0.6372, -0.1144, -0.3496],\n",
      "        [ 0.1696, -0.3503,  0.0459,  ..., -0.2876, -0.0355, -0.2889],\n",
      "        ...,\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237],\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237],\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] glue/mrpc\n",
      "- GLUE, the General Language Understanding Evaluation benchmark\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.180251 | Train acc=0.9571, f1=0.9554 | Valid acc=0.7900, f1=0.7636\n",
      "Epoch 02 | Loss 0.122077 | Train acc=0.9426, f1=0.9387 | Valid acc=0.7611, f1=0.7130\n",
      "Epoch 03 | Loss 0.121944 | Train acc=0.9545, f1=0.9520 | Valid acc=0.8150, f1=0.7818\n",
      "Epoch 04 | Loss 0.138222 | Train acc=0.9623, f1=0.9607 | Valid acc=0.7630, f1=0.7432\n",
      "Epoch 05 | Loss 0.126284 | Train acc=0.9293, f1=0.9312 | Valid acc=0.6975, f1=0.7325\n",
      "Epoch 06 | Loss 0.106004 | Train acc=0.9626, f1=0.9614 | Valid acc=0.7977, f1=0.7799\n",
      "Epoch 07 | Loss 0.098842 | Train acc=0.9513, f1=0.9515 | Valid acc=0.7322, f1=0.7540\n",
      "Epoch 08 | Loss 0.109770 | Train acc=0.9038, f1=0.8906 | Valid acc=0.7572, f1=0.6631\n",
      "Epoch 09 | Loss 0.121385 | Train acc=0.9374, f1=0.9382 | Valid acc=0.7033, f1=0.7308\n",
      "Epoch 10 | Loss 0.146029 | Train acc=0.9553, f1=0.9533 | Valid acc=0.7495, f1=0.7246\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_experiment(gpu_id=gpu_id, lang_model=lang_models[lm_id], learning_rate=1e-5, max_seq_length=512, max_epoch=10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter LTN",
   "language": "python",
   "name": "ltn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}