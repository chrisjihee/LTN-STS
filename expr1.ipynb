{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Python Path: /home/chris/miniconda3/envs/LTN/bin/python\n",
      "* Library Version:\n",
      "  - torch 1.10.1\n",
      "  - LTNtorch 0.9\n",
      "  - transformers 4.15.0\n",
      "* Experiment ID: 1\n",
      "* Pretrained Model: skt/kobert-base-v1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from main import lang_models, TrainerForClassification, TrainerForRegression\n",
    "\n",
    "eid = 1\n",
    "python = sys.executable\n",
    "print(f\"* Python Path: {python}\")\n",
    "print(f\"* Library Version:\")\n",
    "!echo '  -' `$python -m pip list | grep -w torch`\n",
    "!echo '  -' `$python -m pip list | grep -w LTNtorch`\n",
    "!echo '  -' `$python -m pip list | grep -w transformers`\n",
    "print(f\"* Experiment ID: {eid}\")\n",
    "print(f\"* Pretrained Model: {lang_models[eid]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression using KoBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning_rate=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-05 00:12:27] [BEGIN] TrainerForRegression.define().train()\n",
      "\n",
      "================================================================================================================\n",
      "[device] cuda:1 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cbefb35d01177284\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-cbefb35d01177284/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9cd699cc72461f84e42701335141a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[tokenizer(KoBERTTokenizer)] PreTrainedTokenizer(name_or_path='skt/kobert-base-v1', vocab_size=8002, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '▁한국', '어', '▁사전', '학습', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']\n",
      "- ids    = [2, 4958, 6855, 2625, 7826, 2046, 7088, 1050, 7843, 54, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1b659411464dec95dc073f7f12d790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= [CLS] ▁숙 소 ▁위치 는 ▁찾기 ▁ 쉽 고 ▁일반 적인 ▁한국의 ▁반 지 하 ▁숙 소 입니다 . [SEP] ▁숙 박 시설 의 ▁위치 ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020c74fb6fd4444f855dd2276466317a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "-      input_ids(2x512) : [2, 4958, 6855, 2625, 7826, 2046, 7088, 1050, 7843, 54, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[-0.2184,  0.0321,  0.1199,  ..., -0.1616,  0.0468,  0.1257],\n",
      "        [ 0.0439, -0.3687, -0.0513,  ..., -0.6372, -0.1144, -0.3496],\n",
      "        [ 0.1696, -0.3503,  0.0459,  ..., -0.2876, -0.0355, -0.2889],\n",
      "        ...,\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237],\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237],\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] pearson, spearmanr\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.026017 | Train pearson=0.9804, spearmanr=0.9431 | Valid pearson=0.8755, spearmanr=0.8827\n",
      "Epoch 02 | Loss 0.016751 | Train pearson=0.9852, spearmanr=0.9585 | Valid pearson=0.8709, spearmanr=0.8794\n",
      "Epoch 03 | Loss 0.014077 | Train pearson=0.9892, spearmanr=0.9675 | Valid pearson=0.8801, spearmanr=0.8790\n",
      "Epoch 04 | Loss 0.012847 | Train pearson=0.9916, spearmanr=0.9740 | Valid pearson=0.8839, spearmanr=0.8808\n",
      "Epoch 05 | Loss 0.011541 | Train pearson=0.9927, spearmanr=0.9775 | Valid pearson=0.8875, spearmanr=0.8846\n",
      "Epoch 06 | Loss 0.010916 | Train pearson=0.9933, spearmanr=0.9801 | Valid pearson=0.8900, spearmanr=0.8877\n",
      "Epoch 07 | Loss 0.009927 | Train pearson=0.9945, spearmanr=0.9809 | Valid pearson=0.8876, spearmanr=0.8828\n",
      "Epoch 08 | Loss 0.009251 | Train pearson=0.9946, spearmanr=0.9828 | Valid pearson=0.8848, spearmanr=0.8839\n",
      "Epoch 09 | Loss 0.009248 | Train pearson=0.9948, spearmanr=0.9825 | Valid pearson=0.8875, spearmanr=0.8824\n",
      "Epoch 10 | Loss 0.008769 | Train pearson=0.9955, spearmanr=0.9845 | Valid pearson=0.8825, spearmanr=0.8855\n",
      "[2022-01-05 01:01:42] [END] TrainerForRegression.define().train()\n"
     ]
    }
   ],
   "source": [
    "!date +\"[%Y-%m-%d %H:%I:%S] [BEGIN] TrainerForRegression.define().train()\"\n",
    "TrainerForRegression(gpu_id=eid, lang_model=lang_models[eid], max_epoch=10,\n",
    "                     learning_rate=1e-5, max_seq_length=512).define().train()\n",
    "!date +\"[%Y-%m-%d %H:%I:%S] [END] TrainerForRegression.define().train()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning_rate=2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-05 01:01:43] [BEGIN] TrainerForRegression.define().train()\n",
      "\n",
      "================================================================================================================\n",
      "[device] cuda:1 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cbefb35d01177284\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-cbefb35d01177284/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f769e57d834c6fb9a6799feff227dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[tokenizer(KoBERTTokenizer)] PreTrainedTokenizer(name_or_path='skt/kobert-base-v1', vocab_size=8002, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '▁한국', '어', '▁사전', '학습', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']\n",
      "- ids    = [2, 4958, 6855, 2625, 7826, 2046, 7088, 1050, 7843, 54, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef7fdfc6ff340a6bc3d2d022c80a6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= [CLS] ▁숙 소 ▁위치 는 ▁찾기 ▁ 쉽 고 ▁일반 적인 ▁한국의 ▁반 지 하 ▁숙 소 입니다 . [SEP] ▁숙 박 시설 의 ▁위치 ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65694a0488a045a7bba0611ac9876a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "-      input_ids(2x512) : [2, 4958, 6855, 2625, 7826, 2046, 7088, 1050, 7843, 54, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[-0.2184,  0.0321,  0.1199,  ..., -0.1616,  0.0468,  0.1257],\n",
      "        [ 0.0439, -0.3687, -0.0513,  ..., -0.6372, -0.1144, -0.3496],\n",
      "        [ 0.1696, -0.3503,  0.0459,  ..., -0.2876, -0.0355, -0.2889],\n",
      "        ...,\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237],\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237],\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] pearson, spearmanr\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.025253 | Train pearson=0.9786, spearmanr=0.9426 | Valid pearson=0.8752, spearmanr=0.8755\n",
      "Epoch 02 | Loss 0.017717 | Train pearson=0.9856, spearmanr=0.9574 | Valid pearson=0.8758, spearmanr=0.8786\n",
      "Epoch 03 | Loss 0.014973 | Train pearson=0.9881, spearmanr=0.9656 | Valid pearson=0.8858, spearmanr=0.8814\n",
      "Epoch 04 | Loss 0.013444 | Train pearson=0.9900, spearmanr=0.9697 | Valid pearson=0.8848, spearmanr=0.8844\n",
      "Epoch 05 | Loss 0.012579 | Train pearson=0.9917, spearmanr=0.9722 | Valid pearson=0.8845, spearmanr=0.8846\n",
      "Epoch 06 | Loss 0.011304 | Train pearson=0.9918, spearmanr=0.9755 | Valid pearson=0.8956, spearmanr=0.8940\n",
      "Epoch 07 | Loss 0.011401 | Train pearson=0.9936, spearmanr=0.9779 | Valid pearson=0.8948, spearmanr=0.8940\n",
      "Epoch 08 | Loss 0.010525 | Train pearson=0.9934, spearmanr=0.9789 | Valid pearson=0.8805, spearmanr=0.8906\n",
      "Epoch 09 | Loss 0.010076 | Train pearson=0.9941, spearmanr=0.9810 | Valid pearson=0.8794, spearmanr=0.8807\n",
      "Epoch 10 | Loss 0.009658 | Train pearson=0.9947, spearmanr=0.9813 | Valid pearson=0.8866, spearmanr=0.8856\n",
      "[2022-01-05 03:03:25] [END] TrainerForRegression.define().train()\n"
     ]
    }
   ],
   "source": [
    "!date +\"[%Y-%m-%d %H:%I:%S] [BEGIN] TrainerForRegression.define().train()\"\n",
    "TrainerForRegression(gpu_id=eid, lang_model=lang_models[eid], max_epoch=10,\n",
    "                     learning_rate=2e-5, max_seq_length=512).define().train()\n",
    "!date +\"[%Y-%m-%d %H:%I:%S] [END] TrainerForRegression.define().train()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using KoBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning_rate=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-05 03:03:26] [BEGIN] TrainerForClassification.define().train()\n",
      "\n",
      "================================================================================================================\n",
      "[device] cuda:1 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b3f46ff75b254c98\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-b3f46ff75b254c98/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d77cbd9bd94b719639aad485929d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[tokenizer(KoBERTTokenizer)] PreTrainedTokenizer(name_or_path='skt/kobert-base-v1', vocab_size=8002, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '▁한국', '어', '▁사전', '학습', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']\n",
      "- ids    = [2, 4958, 6855, 2625, 7826, 2046, 7088, 1050, 7843, 54, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031526ed2b0346b5b4ae4c32396f21ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= [CLS] ▁숙 소 ▁위치 는 ▁찾기 ▁ 쉽 고 ▁일반 적인 ▁한국의 ▁반 지 하 ▁숙 소 입니다 . [SEP] ▁숙 박 시설 의 ▁위치 ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d205cf95fb84f2fa38d4697786c05fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "-      input_ids(2x512) : [2, 4958, 6855, 2625, 7826, 2046, 7088, 1050, 7843, 54, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[-0.2184,  0.0321,  0.1199,  ..., -0.1616,  0.0468,  0.1257],\n",
      "        [ 0.0439, -0.3687, -0.0513,  ..., -0.6372, -0.1144, -0.3496],\n",
      "        [ 0.1696, -0.3503,  0.0459,  ..., -0.2876, -0.0355, -0.2889],\n",
      "        ...,\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237],\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237],\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] accuracy, f1\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.193393 | Train accuracy=0.9389, f1=0.9360 | Valid accuracy=0.7958, f1=0.7634\n",
      "Epoch 02 | Loss 0.143373 | Train accuracy=0.8936, f1=0.8783 | Valid accuracy=0.7245, f1=0.6166\n",
      "Epoch 03 | Loss 0.134487 | Train accuracy=0.9519, f1=0.9498 | Valid accuracy=0.8131, f1=0.7877\n",
      "Epoch 04 | Loss 0.125612 | Train accuracy=0.9523, f1=0.9497 | Valid accuracy=0.7958, f1=0.7558\n",
      "Epoch 05 | Loss 0.128093 | Train accuracy=0.9292, f1=0.9295 | Valid accuracy=0.7611, f1=0.7660\n",
      "Epoch 06 | Loss 0.126474 | Train accuracy=0.9553, f1=0.9531 | Valid accuracy=0.7765, f1=0.7532\n",
      "Epoch 07 | Loss 0.118537 | Train accuracy=0.9590, f1=0.9578 | Valid accuracy=0.7592, f1=0.7505\n",
      "Epoch 08 | Loss 0.149902 | Train accuracy=0.9459, f1=0.9425 | Valid accuracy=0.7803, f1=0.7385\n",
      "Epoch 09 | Loss 0.121544 | Train accuracy=0.9525, f1=0.9524 | Valid accuracy=0.7534, f1=0.7681\n",
      "Epoch 10 | Loss 0.114835 | Train accuracy=0.9636, f1=0.9629 | Valid accuracy=0.7842, f1=0.7838\n",
      "[2022-01-05 04:04:48] [END] TrainerForClassification.define().train()\n"
     ]
    }
   ],
   "source": [
    "!date +\"[%Y-%m-%d %H:%I:%S] [BEGIN] TrainerForClassification.define().train()\"\n",
    "TrainerForClassification(gpu_id=eid, lang_model=lang_models[eid], max_epoch=10,\n",
    "                         learning_rate=1e-5, max_seq_length=512).define().train()\n",
    "!date +\"[%Y-%m-%d %H:%I:%S] [END] TrainerForClassification.define().train()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning_rate=2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-05 04:04:48] [BEGIN] TrainerForClassification.define().train()\n",
      "\n",
      "================================================================================================================\n",
      "[device] cuda:1 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b3f46ff75b254c98\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-b3f46ff75b254c98/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12f6902fc1e48aab4a28cecb484d58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[tokenizer(KoBERTTokenizer)] PreTrainedTokenizer(name_or_path='skt/kobert-base-v1', vocab_size=8002, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '▁한국', '어', '▁사전', '학습', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']\n",
      "- ids    = [2, 4958, 6855, 2625, 7826, 2046, 7088, 1050, 7843, 54, 3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f579b67e4db44ba186e77c4fe5df489c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= [CLS] ▁숙 소 ▁위치 는 ▁찾기 ▁ 쉽 고 ▁일반 적인 ▁한국의 ▁반 지 하 ▁숙 소 입니다 . [SEP] ▁숙 박 시설 의 ▁위치 ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45040fe1d28460a8537ddce59f85699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "-      input_ids(2x512) : [2, 4958, 6855, 2625, 7826, 2046, 7088, 1050, 7843, 54, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[-0.2184,  0.0321,  0.1199,  ..., -0.1616,  0.0468,  0.1257],\n",
      "        [ 0.0439, -0.3687, -0.0513,  ..., -0.6372, -0.1144, -0.3496],\n",
      "        [ 0.1696, -0.3503,  0.0459,  ..., -0.2876, -0.0355, -0.2889],\n",
      "        ...,\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237],\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237],\n",
      "        [-0.4164, -0.1198, -0.2127,  ..., -0.4124, -0.1018,  0.1237]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] accuracy, f1\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.205747 | Train accuracy=0.9369, f1=0.9374 | Valid accuracy=0.7437, f1=0.7551\n",
      "Epoch 02 | Loss 0.162031 | Train accuracy=0.4807, f1=0.6490 | Valid accuracy=0.4239, f1=0.5954\n",
      "Epoch 03 | Loss 0.228932 | Train accuracy=0.8930, f1=0.8993 | Valid accuracy=0.6590, f1=0.7055\n",
      "Epoch 04 | Loss 0.154376 | Train accuracy=0.9302, f1=0.9318 | Valid accuracy=0.7187, f1=0.7448\n",
      "Epoch 05 | Loss 0.141436 | Train accuracy=0.9396, f1=0.9363 | Valid accuracy=0.8035, f1=0.7723\n",
      "Epoch 06 | Loss 0.166734 | Train accuracy=0.8867, f1=0.8942 | Valid accuracy=0.6493, f1=0.7036\n",
      "Epoch 07 | Loss 0.178702 | Train accuracy=0.6240, f1=0.3589 | Valid accuracy=0.5954, f1=0.1463\n",
      "Epoch 08 | Loss 0.164158 | Train accuracy=0.9325, f1=0.9272 | Valid accuracy=0.7341, f1=0.6730\n",
      "Epoch 09 | Loss 0.494340 | Train accuracy=0.5082, f1=0.4669 | Valid accuracy=0.5145, f1=0.4425\n",
      "Epoch 10 | Loss 0.501182 | Train accuracy=0.4939, f1=0.5415 | Valid accuracy=0.4663, f1=0.4936\n",
      "[2022-01-05 06:06:18] [END] TrainerForClassification.define().train()\n"
     ]
    }
   ],
   "source": [
    "!date +\"[%Y-%m-%d %H:%I:%S] [BEGIN] TrainerForClassification.define().train()\"\n",
    "TrainerForClassification(gpu_id=eid, lang_model=lang_models[eid], max_epoch=10,\n",
    "                         learning_rate=2e-5, max_seq_length=512).define().train()\n",
    "!date +\"[%Y-%m-%d %H:%I:%S] [END] TrainerForClassification.define().train()\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter LTN",
   "language": "python",
   "name": "ltn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}