{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Python path: /home/chris/miniconda3/envs/LTN/bin/python\n",
      "* Library version:\n",
      "  - numpy 1.21.5\r\n",
      "  - datasets 1.17.0\r\n",
      "  - torch 1.10.1\r\n",
      "  - LTNtorch 0.9\r\n",
      "  - transformers 4.15.0\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "python = sys.executable\n",
    "print(f\"* Python path: {python}\")\n",
    "print(f\"* Library version:\")\n",
    "!echo '  -' `$python -m pip list | grep -w numpy`\n",
    "!echo '  -' `$python -m pip list | grep -w datasets`\n",
    "!echo '  -' `$python -m pip list | grep -w torch`\n",
    "!echo '  -' `$python -m pip list | grep -w LTNtorch`\n",
    "!echo '  -' `$python -m pip list | grep -w transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* data_files:\n",
      "  - train: data/klue-sts-cls/train.json\n",
      "  - valid: data/klue-sts-cls/valid.json\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "total 3764\r\n",
      "-rw-rw-r-- 1 chris chris 3690197 Dec 30 16:55 train.json\r\n",
      "-rw-rw-r-- 1 chris chris  160929 Dec 30 16:55 valid.json\r\n"
     ]
    }
   ],
   "source": [
    "from main import data_files\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(data_files['train']).parent\n",
    "\n",
    "print(f\"* data_files:\")\n",
    "for k, v in data_files.items():\n",
    "    print(f\"  - {k}: {v}\")\n",
    "print('-' * 112)\n",
    "!ls -l $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Env setting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* available gpu_ids: (0, 1, 2, 3)\n",
      "* gpu_id: 0\n",
      "* available lang_models: \n",
      "  - bert-base-multilingual-uncased\n",
      "  - skt/kobert-base-v1\n",
      "  - monologg/koelectra-base-v3-discriminator\n",
      "  - monologg/kobigbird-bert-base\n",
      "* lang_model: bert-base-multilingual-uncased\n"
     ]
    }
   ],
   "source": [
    "from main import gpu_ids, lang_models, do_experiment\n",
    "\n",
    "gpu_id = 0\n",
    "print(f\"* available gpu_ids: {gpu_ids}\")\n",
    "print(f\"* gpu_id: {gpu_id}\")\n",
    "\n",
    "lm_id = 0\n",
    "print(f\"* available lang_models: \")\n",
    "for m in lang_models:\n",
    "    print(f\"  - {m}\")\n",
    "print(f\"* lang_model: {lang_models[lm_id]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KLUE-STS(cls) [mBERT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[device] cuda:0 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b3f46ff75b254c98\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-b3f46ff75b254c98/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28f2b45bc3f64420b77024322b457b40"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[tokenizer(BertTokenizerFast)] PreTrainedTokenizerFast(name_or_path='bert-base-multilingual-uncased', vocab_size=105879, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '한국', '##어', 'ᄉ', '##ᅡ', '##전', '##학', '##스', '##ᆸ', 'ᄆ', '##ᅩ', '##데', '##ᆯ을', '고', '##ᆼ', '##유', '##합', '##니다', '.', '[SEP]']\n",
      "- ids    = [101, 47529, 13413, 1172, 25539, 17101, 21596, 13212, 17360, 1169, 29347, 26872, 22058, 47468, 13045, 42159, 44859, 82566, 119, 102]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "295cafce252a47ee8e0be510993de780"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= ['[CLS]', '수', '##ᆨ', '##소', 'ᄋ', '##ᅱ', '##치는', 'ᄎ', '##ᅡ', '##ᆽ', '##기', 'ᄉ', '##ᅱ', '##ᆸ고', '이', '##ᆯ', '##반', '##적인', '한국', '##의', 'ᄇ', '##ᅡᆫ', '##지', '##하', '수', '...', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5da1d5f32e094518826956b3260d6faa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "-      input_ids(2x512) : [101, 47529, 13413, 1172, 25539, 17101, 21596, 13212, 17360, 1169, 29347, 26872, 22058, 47468, 13045, 42159, 44859, 82566, 119, 102, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[-0.0510, -0.1183, -0.0165,  ...,  0.0060, -0.0585, -0.1084],\n",
      "        [ 0.3172,  0.0977, -0.1539,  ...,  0.0561,  0.4085, -0.9275],\n",
      "        [ 0.2552, -0.2561, -0.2972,  ..., -0.2263,  0.8786, -0.2729],\n",
      "        ...,\n",
      "        [ 0.0233,  0.1671, -0.2421,  ...,  0.2711, -0.4352, -0.2061],\n",
      "        [ 0.1050,  0.1724, -0.1510,  ...,  0.1889, -0.1483, -0.2658],\n",
      "        [ 0.1774,  0.1179, -0.1413,  ...,  0.1362,  0.0420, -0.2168]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] glue/mrpc\n",
      "- GLUE, the General Language Understanding Evaluation benchmark\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.262456 | Train acc=0.9002, f1=0.8953 | Valid acc=0.6879, f1=0.6524\n",
      "Epoch 02 | Loss 0.242155 | Train acc=0.8638, f1=0.8441 | Valid acc=0.6243, f1=0.5185\n",
      "Epoch 03 | Loss 0.280823 | Train acc=0.9045, f1=0.8999 | Valid acc=0.6802, f1=0.6391\n",
      "Epoch 04 | Loss 0.331647 | Train acc=0.7473, f1=0.7914 | Valid acc=0.4933, f1=0.6205\n",
      "Epoch 05 | Loss 0.276388 | Train acc=0.8735, f1=0.8815 | Valid acc=0.6435, f1=0.6880\n",
      "Epoch 06 | Loss 0.331358 | Train acc=0.8690, f1=0.8514 | Valid acc=0.6628, f1=0.5614\n",
      "Epoch 07 | Loss 0.309101 | Train acc=0.4958, f1=0.5014 | Valid acc=0.5010, f1=0.4682\n",
      "Epoch 08 | Loss 0.462873 | Train acc=0.8094, f1=0.8342 | Valid acc=0.5434, f1=0.6457\n",
      "Epoch 09 | Loss 0.452130 | Train acc=0.5127, f1=0.4709 | Valid acc=0.5048, f1=0.4276\n",
      "Epoch 10 | Loss 0.474318 | Train acc=0.5696, f1=0.4406 | Valid acc=0.5703, f1=0.3957\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_experiment(gpu_id=gpu_id, lang_model=lang_models[lm_id], learning_rate=2e-5, max_seq_length=512, max_epoch=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[device] cuda:0 ∈ [cuda:0, cuda:1, cuda:2, cuda:3]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b3f46ff75b254c98\n",
      "Reusing dataset json (/home/chris/.cache/huggingface/datasets/json/default-b3f46ff75b254c98/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ed6e4afdc5847be96bc72831c0372d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[raw_datasets] DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 11668\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['guid', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 519\n",
      "    })\n",
      "})\n",
      "- input_columns: sentence1, sentence2\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[tokenizer(BertTokenizerFast)] PreTrainedTokenizerFast(name_or_path='bert-base-multilingual-uncased', vocab_size=105879, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "- text   = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]\n",
      "- tokens = ['[CLS]', '한국', '##어', 'ᄉ', '##ᅡ', '##전', '##학', '##스', '##ᆸ', 'ᄆ', '##ᅩ', '##데', '##ᆯ을', '고', '##ᆼ', '##유', '##합', '##니다', '.', '[SEP]']\n",
      "- ids    = [101, 47529, 13413, 1172, 25539, 17101, 21596, 13212, 17360, 1169, 29347, 26872, 22058, 47468, 13045, 42159, 44859, 82566, 119, 102]\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01e2f140339342febab332597ad4b348"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "- [tokens](512)\t= ['[CLS]', '수', '##ᆨ', '##소', 'ᄋ', '##ᅱ', '##치는', 'ᄎ', '##ᅡ', '##ᆽ', '##기', 'ᄉ', '##ᅱ', '##ᆸ고', '이', '##ᆯ', '##반', '##적인', '한국', '##의', 'ᄇ', '##ᅡᆫ', '##지', '##하', '수', '...', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cba6f9b011cc460b97b7a0bbe0b8ae6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================\n",
      "[pretrained] BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  ...\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "-      input_ids(2x512) : [101, 47529, 13413, 1172, 25539, 17101, 21596, 13212, 17360, 1169, 29347, 26872, 22058, 47468, 13045, 42159, 44859, 82566, 119, 102, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- attention_mask(2x512) : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "- token_type_ids(2x512) : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-  output_hidden(2x512x768) : tensor([[-0.0510, -0.1183, -0.0165,  ...,  0.0060, -0.0585, -0.1084],\n",
      "        [ 0.3172,  0.0977, -0.1539,  ...,  0.0561,  0.4085, -0.9275],\n",
      "        [ 0.2552, -0.2561, -0.2972,  ..., -0.2263,  0.8786, -0.2729],\n",
      "        ...,\n",
      "        [ 0.0233,  0.1671, -0.2421,  ...,  0.2711, -0.4352, -0.2061],\n",
      "        [ 0.1050,  0.1724, -0.1510,  ...,  0.1889, -0.1483, -0.2658],\n",
      "        [ 0.1774,  0.1179, -0.1413,  ...,  0.1362,  0.0420, -0.2168]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "================================================================================================================\n",
      "[metric] glue/mrpc\n",
      "- GLUE, the General Language Understanding Evaluation benchmark\n",
      "================================================================================================================\n",
      "\n",
      "Epoch 01 | Loss 0.217675 | Train acc=0.9341, f1=0.9343 | Valid acc=0.6358, f1=0.6607\n",
      "Epoch 02 | Loss 0.180642 | Train acc=0.9455, f1=0.9435 | Valid acc=0.6936, f1=0.6694\n",
      "Epoch 03 | Loss 0.163123 | Train acc=0.9425, f1=0.9397 | Valid acc=0.6570, f1=0.6229\n",
      "Epoch 04 | Loss 0.152514 | Train acc=0.9464, f1=0.9440 | Valid acc=0.7148, f1=0.6711\n",
      "Epoch 05 | Loss 0.141988 | Train acc=0.9429, f1=0.9426 | Valid acc=0.6956, f1=0.7030\n",
      "Epoch 06 | Loss 0.157464 | Train acc=0.9415, f1=0.9383 | Valid acc=0.7033, f1=0.6500\n",
      "Epoch 07 | Loss 0.196250 | Train acc=0.8860, f1=0.8931 | Valid acc=0.5915, f1=0.6592\n",
      "Epoch 08 | Loss 0.234037 | Train acc=0.8568, f1=0.8695 | Valid acc=0.6513, f1=0.7038\n",
      "Epoch 09 | Loss 0.206662 | Train acc=0.8707, f1=0.8480 | Valid acc=0.6994, f1=0.5412\n",
      "Epoch 10 | Loss 0.196285 | Train acc=0.9104, f1=0.9006 | Valid acc=0.6936, f1=0.5805\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_experiment(gpu_id=gpu_id, lang_model=lang_models[lm_id], learning_rate=1e-5, max_seq_length=512, max_epoch=10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter LTN",
   "language": "python",
   "name": "ltn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}